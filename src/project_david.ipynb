{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "639b2047",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10256428",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb4a9463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import copy\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61155c8",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29cabdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 5378661\n",
      "Unique characters (K): 106\n",
      "Sample char to index mapping: [('\\t', 0), ('\\n', 1), (' ', 2), ('!', 3), ('#', 4), ('$', 5), ('%', 6), ('&', 7), (\"'\", 8), ('(', 9)]\n"
     ]
    }
   ],
   "source": [
    "fname = '../data/shaketext.txt'\n",
    "\n",
    "with open(fname, \"r\") as fid:\n",
    "    data = fid.read()\n",
    "\n",
    "unique_chars = list(set(data))\n",
    "K = len(unique_chars)\n",
    "unique_chars_sorted = sorted(unique_chars)\n",
    "\n",
    "char_to_index = {char: index for index, char in enumerate(unique_chars_sorted)}\n",
    "index_to_char = {index: char for index, char in enumerate(unique_chars_sorted)}\n",
    "\n",
    "print(\"Total characters:\", len(data))\n",
    "print(\"Unique characters (K):\", K)\n",
    "print(\"Sample char to index mapping:\", list(char_to_index.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d816c5",
   "metadata": {},
   "source": [
    "## Baseline RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e7021f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = [char_to_index[c] for c in data]\n",
    "\n",
    "seq_length = 100\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58d44691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: (64, 100)\n",
      "Target batch shape: (64, 100)\n",
      "Decoded input: r true image pictured lies,\n",
      "Which in my bosom’s shop is hanging still,\n",
      "That hath his windows glazed \n",
      "Decoded target:  true image pictured lies,\n",
      "Which in my bosom’s shop is hanging still,\n",
      "That hath his windows glazed w\n"
     ]
    }
   ],
   "source": [
    "for x_batch, y_batch in dataset.take(1):\n",
    "    print(\"Input batch shape:\", x_batch.shape)\n",
    "    print(\"Target batch shape:\", y_batch.shape)\n",
    "    first_input = ''.join(index_to_char[idx] for idx in x_batch[0].numpy())\n",
    "    first_target = ''.join(index_to_char[idx] for idx in y_batch[0].numpy())\n",
    "    print(\"Decoded input:\", first_input)\n",
    "    print(\"Decoded target:\", first_target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9316c064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text pre-training:\n",
      "ROMEO.BJ:H_k'L49\n",
      "™%G1Di‘Bv-.&hYÆOBy&2[a\t;àh’É1sHk “dîVxRpa&kE'To,àgré#QX7S/0Id/•Ph3I\n",
      ";$t&3zhë\tx…?êÆ)BÉjæv(ÇAl9gMXâÀUêA;U8’[Wæ#KcPDgÇfmGâg,/xJkI\tY_ê_A)u/Yy]!oÆz—‘f\n",
      "Àè(‘X/qj/ÆOêVÇK]LKQfè4v;!QUëUV\n",
      "c’zM\n",
      "JY/XX;?27%i m7d$,OVpO’%j(/dT‘àê’V*•zj•ÉëDQsR9nhZPê\tlz,“WëE…çyâT0èQ%\n",
      "G1\tD‘/_SxZYc1ë)iî)f!3S*ZXvV]vB),ÀéVç;ëq\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 30ms/step - loss: 2.7347\n",
      "Epoch 2/10\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 30ms/step - loss: 1.9587\n",
      "Epoch 3/10\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - loss: 1.8444\n",
      "Epoch 4/10\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 29ms/step - loss: 1.7886\n",
      "Epoch 5/10\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 30ms/step - loss: 1.7540\n",
      "Epoch 6/10\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 31ms/step - loss: 1.7302\n",
      "Epoch 7/10\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 29ms/step - loss: 1.7145\n",
      "Epoch 8/10\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 29ms/step - loss: 1.7016\n",
      "Epoch 9/10\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 30ms/step - loss: 1.6907\n",
      "Epoch 10/10\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 29ms/step - loss: 1.6816\n",
      "\n",
      "Generated text post-training:\n",
      "ROMEO.\n",
      "The\n",
      "\n",
      "! by’sw Thited nclan igray anicequt thed:\n",
      "TH ton bluplout shencoproutwiges.\n",
      "MILYes orase y llder.\n",
      " mige Pat cay om,\n",
      "APLer are, t ssa geak tesit wig azergad, lskimatinin hopan we y, inis o bengese alot De’ Secor t ARine’T s []\n",
      " m lone kn y T. ounok\n",
      "MUThels plily,\n",
      "SHoxayoumingusinoncu Engre Thes\n"
     ]
    }
   ],
   "source": [
    "rnn_units = 100\n",
    "embedding_dim = rnn_units//2\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(input_dim=K, output_dim=embedding_dim),\n",
    "    layers.SimpleRNN(rnn_units, return_sequences=True),\n",
    "    layers.Dense(K)\n",
    "])\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss_fn)\n",
    "\n",
    "def sample(model, start_string, generation_length=500, temperature=1.0):\n",
    "    input_eval = [char_to_index[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    generated = []\n",
    "\n",
    "    for _ in range(generation_length):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0) / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        generated.append(index_to_char[predicted_id])\n",
    "\n",
    "    return start_string + ''.join(generated)\n",
    "\n",
    "print('Generated text pre-training:')\n",
    "print(sample(model, start_string=\"ROMEO.\", generation_length=300))\n",
    "print()\n",
    "\n",
    "EPOCHS = 10\n",
    "history = model.fit(dataset, epochs=EPOCHS)\n",
    "\n",
    "model.save_weights('../data/baseline_rnn.weights.h5')\n",
    "\n",
    "print()\n",
    "print('Generated text post-training:')\n",
    "print(sample(model, start_string=\"ROMEO.\", generation_length=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3495ee25",
   "metadata": {},
   "source": [
    "## Implementing an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "595b45fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "m = rnn_units\n",
    "rng = np.random.default_rng()\n",
    "BitGen = type(rng.bit_generator)\n",
    "seed = 42\n",
    "rng.bit_generator.state = BitGen(seed).state\n",
    "\n",
    "def initialize_lstm(L, m):\n",
    "    layers = []\n",
    "    for l in range(L):\n",
    "        in_dim = K if l == 0 else m\n",
    "        layer = {\n",
    "          'Wf': (1/np.sqrt(2*m))*rng.standard_normal((m, m)),\n",
    "          'Uf': (1/np.sqrt(2*in_dim))*rng.standard_normal((m, in_dim)),\n",
    "          'bf': np.zeros((m,1)),\n",
    "          'Wi': (1/np.sqrt(2*m))*rng.standard_normal((m, m)),\n",
    "          'Ui': (1/np.sqrt(2*in_dim))*rng.standard_normal((m, in_dim)),\n",
    "          'bi': np.zeros((m,1)),\n",
    "          'Wc': (1/np.sqrt(2*m))*rng.standard_normal((m, m)),\n",
    "          'Uc': (1/np.sqrt(2*in_dim))*rng.standard_normal((m, in_dim)),\n",
    "          'bc': np.zeros((m,1)),\n",
    "          'Wo': (1/np.sqrt(2*m))*rng.standard_normal((m, m)),\n",
    "          'Uo': (1/np.sqrt(2*in_dim))*rng.standard_normal((m, in_dim)),\n",
    "          'bo': np.zeros((m,1))\n",
    "        }\n",
    "        layers.append(layer)\n",
    "    # output layer\n",
    "    V = (1/np.sqrt(m))*rng.standard_normal((K, m))\n",
    "    c = np.zeros((K,1))\n",
    "    return {'layers': layers, 'V': V, 'c': c}\n",
    "\n",
    "def fp_lstm(RNN, X, Y, h0s, c0s):\n",
    "    layers = RNN['layers']\n",
    "    V, c_out = RNN['V'], RNN['c']\n",
    "    L = len(layers)\n",
    "\n",
    "    # prepare storages\n",
    "    f = [ [None]*seq_length for _ in range(L) ]\n",
    "    i = [ [None]*seq_length for _ in range(L) ]\n",
    "    c_tilde = [ [None]*seq_length for _ in range(L) ]\n",
    "    o = [ [None]*seq_length for _ in range(L) ]\n",
    "    c_states = [ [None]*(seq_length+1) for _ in range(L) ]\n",
    "    h_states = [ [None]*(seq_length+1) for _ in range(L) ]\n",
    "    y_logits, p = [], []\n",
    "\n",
    "    # init\n",
    "    for l in range(L):\n",
    "        h_states[l][0] = h0s[l]\n",
    "        c_states[l][0] = c0s[l]\n",
    "\n",
    "    loss = 0\n",
    "    # time-step loop\n",
    "    for t in range(seq_length):\n",
    "        # input to layer 0\n",
    "        x_in = X[:, t:t+1]    # shape (K,1)\n",
    "\n",
    "        # forward through L layers\n",
    "        for l in range(L):\n",
    "            Wf, Uf, bf = layers[l]['Wf'], layers[l]['Uf'], layers[l]['bf']\n",
    "            Wi, Ui, bi = layers[l]['Wi'], layers[l]['Ui'], layers[l]['bi']\n",
    "            Wc, Uc, bc = layers[l]['Wc'], layers[l]['Uc'], layers[l]['bc']\n",
    "            Wo, Uo, bo = layers[l]['Wo'], layers[l]['Uo'], layers[l]['bo']\n",
    "\n",
    "            h_prev = h_states[l][t]\n",
    "            c_prev = c_states[l][t]\n",
    "\n",
    "            # gates\n",
    "            fl = 1/(1+np.exp(-(Wf@h_prev + Uf@x_in + bf)))\n",
    "            il = 1/(1+np.exp(-(Wi@h_prev + Ui@x_in + bi)))\n",
    "            cbarl = np.tanh(   Wc@h_prev + Uc@x_in + bc)\n",
    "            cl = fl*c_prev + il*cbarl\n",
    "            ol = 1/(1+np.exp(-(Wo@h_prev + Uo@x_in + bo)))\n",
    "            hl = ol * np.tanh(cl)\n",
    "\n",
    "            # stash\n",
    "            f[l][t] = fl;    i[l][t] = il\n",
    "            c_tilde[l][t] = cbarl; o[l][t] = ol\n",
    "            c_states[l][t+1] = cl; h_states[l][t+1] = hl\n",
    "\n",
    "            # next layer’s input\n",
    "            x_in = hl\n",
    "\n",
    "        # output & loss\n",
    "        logit = V @ h_states[L-1][t+1] + c_out\n",
    "        exp_l = np.exp(logit - np.max(logit))\n",
    "        p_t = exp_l/np.sum(exp_l)\n",
    "        loss += -np.log(Y[:,t:t+1].T @ p_t)\n",
    "\n",
    "        y_logits.append(logit); p.append(p_t)\n",
    "\n",
    "    cache = {\n",
    "      'f': f, 'i': i, 'c_tilde': c_tilde, 'o': o,\n",
    "      'c_states': c_states, 'h_states': h_states,\n",
    "      'y_logits': y_logits, 'p': p\n",
    "    }\n",
    "\n",
    "    return cache, loss[0,0]/seq_length\n",
    "\n",
    "def bp_lstm(RNN, X, Y, cache):\n",
    "    layers = RNN['layers']\n",
    "    V, c_out = RNN['V'], RNN['c']\n",
    "    L = len(layers)\n",
    "\n",
    "    # unpack cache\n",
    "    f = cache['f']; i = cache['i']; cbar = cache['c_tilde']; o = cache['o']\n",
    "    c_states = cache['c_states']; h_states = cache['h_states']\n",
    "    p = cache['p']\n",
    "\n",
    "    # prepare gradients\n",
    "    grads = { 'layers': [], 'V': np.zeros_like(V), 'c': np.zeros_like(c_out) }\n",
    "    for l in range(L):\n",
    "        in_dim = K if l == 0 else m\n",
    "        grads['layers'].append({\n",
    "          'Wf':np.zeros((m,m)), 'Uf':np.zeros((m,in_dim)), 'bf':np.zeros((m,1)),\n",
    "          'Wi':np.zeros((m,m)), 'Ui':np.zeros((m,in_dim)), 'bi':np.zeros((m,1)),\n",
    "          'Wc':np.zeros((m,m)), 'Uc':np.zeros((m,in_dim)), 'bc':np.zeros((m,1)),\n",
    "          'Wo':np.zeros((m,m)), 'Uo':np.zeros((m,in_dim)), 'bo':np.zeros((m,1))\n",
    "        })\n",
    "\n",
    "    # time-step and BPTT buffers per layer\n",
    "    dh_time = [np.zeros((m,1)) for _ in range(L)]\n",
    "    dc_time = [np.zeros((m,1)) for _ in range(L)]\n",
    "\n",
    "    # reverse time loop\n",
    "    for t in reversed(range(seq_length)):\n",
    "        # — output layer —\n",
    "        dy = p[t] - Y[:,t:t+1]\n",
    "        grads['V'] += dy @ h_states[L-1][t+1].T\n",
    "        grads['c'] += dy\n",
    "\n",
    "        # seed up-stream grad into top layer\n",
    "        dh_time[L-1] += V.T @ dy\n",
    "\n",
    "        # now backprop through layers l=L-1…0\n",
    "        dh_down = None\n",
    "        for l in reversed(range(L)):\n",
    "            Wf, Uf, bf = layers[l]['Wf'], layers[l]['Uf'], layers[l]['bf']\n",
    "            Wi, Ui, bi = layers[l]['Wi'], layers[l]['Ui'], layers[l]['bi']\n",
    "            Wc, Uc, bc = layers[l]['Wc'], layers[l]['Uc'], layers[l]['bc']\n",
    "            Wo, Uo, bo = layers[l]['Wo'], layers[l]['Uo'], layers[l]['bo']\n",
    "\n",
    "            # gather forward caches\n",
    "            fl = f[l][t]; il = i[l][t]; cbarl = cbar[l][t]; ol = o[l][t]\n",
    "            c_prev = c_states[l][t]; c_curr = c_states[l][t+1]\n",
    "            h_prev = h_states[l][t]; h_curr = h_states[l][t+1]\n",
    "\n",
    "            # total dh into this layer = dh from next time-step + dh from above-layer\n",
    "            dh = dh_time[l]\n",
    "            # dc from next time-step\n",
    "            dc = dc_time[l]\n",
    "\n",
    "            # --- gate gradients ---\n",
    "            dao = dh * np.tanh(c_curr)\n",
    "            dao_raw = dao * ol*(1-ol)\n",
    "\n",
    "            dc_tot = dh*ol*(1-np.tanh(c_curr)**2) + dc\n",
    "\n",
    "            daf = dc_tot * c_prev\n",
    "            daf_raw = daf * fl*(1-fl)\n",
    "\n",
    "            dai = dc_tot * cbarl\n",
    "            dai_raw = dai * il*(1-il)\n",
    "\n",
    "            dac = dc_tot * il\n",
    "            dac_raw = dac * (1-cbarl**2)\n",
    "\n",
    "            # accumulate grads\n",
    "            # input to this layer at time t:\n",
    "            x_in = X[:,t:t+1] if l==0 else h_states[l-1][t+1]\n",
    "\n",
    "            gl = grads['layers'][l]\n",
    "            gl['Wf'] += daf_raw @ h_prev.T\n",
    "            gl['Uf'] += daf_raw @ x_in.T\n",
    "            gl['bf'] += daf_raw\n",
    "\n",
    "            gl['Wi'] += dai_raw @ h_prev.T\n",
    "            gl['Ui'] += dai_raw @ x_in.T\n",
    "            gl['bi'] += dai_raw\n",
    "\n",
    "            gl['Wc'] += dac_raw @ h_prev.T\n",
    "            gl['Uc'] += dac_raw @ x_in.T\n",
    "            gl['bc'] += dac_raw\n",
    "\n",
    "            gl['Wo'] += dao_raw @ h_prev.T\n",
    "            gl['Uo'] += dao_raw @ x_in.T\n",
    "            gl['bo'] += dao_raw\n",
    "\n",
    "            # --- propagate to previous time-step for same layer ---\n",
    "            dh_time[l] = (Wf.T@daf_raw +\n",
    "                          Wi.T@dai_raw +\n",
    "                          Wc.T@dac_raw +\n",
    "                          Wo.T@dao_raw)\n",
    "            dc_time[l] = dc_tot * fl\n",
    "\n",
    "            # --- propagate down to layer l−1 at same time t ---\n",
    "            if l>0:\n",
    "                dh_time[l-1] += (\n",
    "                  Uf.T@daf_raw +\n",
    "                  Ui.T@dai_raw +\n",
    "                  Uc.T@dac_raw +\n",
    "                  Uo.T@dao_raw\n",
    "                )\n",
    "\n",
    "        # end per-layer loop\n",
    "    # end time-loop\n",
    "\n",
    "    # average over timesteps\n",
    "    for l in range(L):\n",
    "        for k in grads['layers'][l]:\n",
    "            grads['layers'][l][k] /= seq_length\n",
    "    grads['V'] /= seq_length\n",
    "    grads['c'] /= seq_length\n",
    "\n",
    "    return grads\n",
    "\n",
    "def train_lstm_adam(dataset, init_RNN, params, lam = 0):\n",
    "    eta, num_epochs, n_batch = params['eta'], params['num_epochs'], params['n_batch']\n",
    "    beta1, beta2, eps = params['beta1'], params['beta2'], params['eps']\n",
    "\n",
    "    RNN = copy.deepcopy(init_RNN)\n",
    "\n",
    "    L = len(RNN['layers'])\n",
    "    m1 = {'layers': [], 'V': np.zeros_like(RNN['V']), 'c': np.zeros_like(RNN['c'])}\n",
    "    v1 = {'layers': [], 'V': np.zeros_like(RNN['V']), 'c': np.zeros_like(RNN['c'])}\n",
    "    for l in range(L):\n",
    "        zero_buf = {k: np.zeros_like(RNN['layers'][l][k]) for k in RNN['layers'][l]}\n",
    "        m1['layers'].append({**zero_buf})\n",
    "        v1['layers'].append({**zero_buf})\n",
    "\n",
    "    single_seq_ds = dataset.unbatch()\n",
    "\n",
    "    t_step = 0\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        epoch_loss = 0.0\n",
    "        n_seqs = 0\n",
    "\n",
    "        for x_seq, y_seq in tqdm(single_seq_ds):\n",
    "            # x_seq, y_seq are TF Tensors of shape (seq_length,)\n",
    "            x_ids = x_seq.numpy()\n",
    "            y_ids = y_seq.numpy()\n",
    "\n",
    "            '''\n",
    "            x_txt = ''.join(index_to_char[id] for id in x_ids)\n",
    "            y_txt = ''.join(index_to_char[id] for id in y_ids)\n",
    "\n",
    "            print(f\"Sequence {n_seqs}:\")\n",
    "            print(x_txt)\n",
    "            print(y_txt)\n",
    "            print()\n",
    "            '''\n",
    "\n",
    "            # one-hot encode into (K, seq_length)\n",
    "            X = np.zeros((K, seq_length))\n",
    "            Y = np.zeros((K, seq_length))\n",
    "            for t in range(seq_length):\n",
    "                X[x_ids[t], t] = 1\n",
    "                Y[y_ids[t], t] = 1\n",
    "\n",
    "            # zero initial states\n",
    "            h0s = [np.zeros((m,1)) for _ in range(L)]\n",
    "            c0s = [np.zeros((m,1)) for _ in range(L)]\n",
    "\n",
    "            # 5) forward & backward\n",
    "            cache, loss = fp_lstm(RNN, X, Y, h0s, c0s)\n",
    "            grads = bp_lstm(RNN, X, Y, cache)\n",
    "            epoch_loss += loss\n",
    "            n_seqs += 1\n",
    "\n",
    "            # 6) Adam update\n",
    "            t_step += 1\n",
    "            for l in range(L):\n",
    "                for param, g in grads['layers'][l].items():\n",
    "                    m1['layers'][l][param] = beta1*m1['layers'][l][param] + (1-beta1)*g\n",
    "                    v1['layers'][l][param] = beta2*v1['layers'][l][param] + (1-beta2)*(g*g)\n",
    "                    m_hat = m1['layers'][l][param] / (1 - beta1**t_step)\n",
    "                    v_hat = v1['layers'][l][param] / (1 - beta2**t_step)\n",
    "                    RNN['layers'][l][param] -= eta * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "            # — output layer V, c —\n",
    "            for key in ('V','c'):\n",
    "                g = grads[key]\n",
    "                m1[key] = beta1*m1[key] + (1-beta1)*g\n",
    "                v1[key] = beta2*v1[key] + (1-beta2)*(g*g)\n",
    "                m_hat = m1[key] / (1 - beta1**t_step)\n",
    "                v_hat = v1[key] / (1 - beta2**t_step)\n",
    "                RNN[key] -= eta * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "        avg_loss = epoch_loss / n_seqs\n",
    "\n",
    "        print(f\"Epoch {epoch}/{num_epochs} — avg loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return RNN\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sample_text_lstm(RNN, start_string, generation_length, temperature=1.0):\n",
    "    layers = RNN['layers']\n",
    "    V, c_out = RNN['V'], RNN['c']\n",
    "    L = len(layers)\n",
    "\n",
    "    # 1) initialize hidden+cell for each layer to zeros\n",
    "    h_states = [np.zeros((m,1)) for _ in range(L)]\n",
    "    c_states = [np.zeros((m,1)) for _ in range(L)]\n",
    "\n",
    "    # 2) prime with start_string (no sampling yet)\n",
    "    last_char_id = None\n",
    "    for ch in start_string:\n",
    "        x = np.zeros((K,1))\n",
    "        last_char_id = char_to_index[ch]\n",
    "        x[last_char_id,0] = 1\n",
    "\n",
    "        # run through each LSTM layer\n",
    "        for l in range(L):\n",
    "            Wf, Uf, bf = layers[l]['Wf'], layers[l]['Uf'], layers[l]['bf']\n",
    "            Wi, Ui, bi = layers[l]['Wi'], layers[l]['Ui'], layers[l]['bi']\n",
    "            Wc, Uc, bc = layers[l]['Wc'], layers[l]['Uc'], layers[l]['bc']\n",
    "            Wo, Uo, bo = layers[l]['Wo'], layers[l]['Uo'], layers[l]['bo']\n",
    "\n",
    "            h_prev = h_states[l]\n",
    "            c_prev = c_states[l]\n",
    "\n",
    "            f_gate   = sigmoid( Wf @ h_prev + Uf @ x + bf )\n",
    "            i_gate   = sigmoid( Wi @ h_prev + Ui @ x + bi )\n",
    "            c_tilde  =       np.tanh( Wc @ h_prev + Uc @ x + bc )\n",
    "            c_curr   = f_gate * c_prev + i_gate * c_tilde\n",
    "            o_gate   = sigmoid( Wo @ h_prev + Uo @ x + bo )\n",
    "            h_curr   = o_gate * np.tanh(c_curr)\n",
    "\n",
    "            # stash and pass to next layer\n",
    "            h_states[l] = h_curr\n",
    "            c_states[l] = c_curr\n",
    "            x = h_curr\n",
    "\n",
    "    # 3) now generate new chars\n",
    "    generated = []\n",
    "    for _ in range(generation_length):\n",
    "        # one-hot last char as input\n",
    "        x = np.zeros((K,1))\n",
    "        x[last_char_id,0] = 1\n",
    "\n",
    "        # forward through L layers\n",
    "        for l in range(L):\n",
    "            Wf, Uf, bf = layers[l]['Wf'], layers[l]['Uf'], layers[l]['bf']\n",
    "            Wi, Ui, bi = layers[l]['Wi'], layers[l]['Ui'], layers[l]['bi']\n",
    "            Wc, Uc, bc = layers[l]['Wc'], layers[l]['Uc'], layers[l]['bc']\n",
    "            Wo, Uo, bo = layers[l]['Wo'], layers[l]['Uo'], layers[l]['bo']\n",
    "\n",
    "            h_prev = h_states[l]\n",
    "            c_prev = c_states[l]\n",
    "\n",
    "            f_gate   = sigmoid( Wf @ h_prev + Uf @ x + bf )\n",
    "            i_gate   = sigmoid( Wi @ h_prev + Ui @ x + bi )\n",
    "            c_tilde  =       np.tanh( Wc @ h_prev + Uc @ x + bc )\n",
    "            c_curr   = f_gate * c_prev + i_gate * c_tilde\n",
    "            o_gate   = sigmoid( Wo @ h_prev + Uo @ x + bo )\n",
    "            h_curr   = o_gate * np.tanh(c_curr)\n",
    "\n",
    "            h_states[l] = h_curr\n",
    "            c_states[l] = c_curr\n",
    "            x = h_curr\n",
    "\n",
    "        # output layer + sampling\n",
    "        logits = (V @ h_curr + c_out).flatten() / temperature\n",
    "        exp_logits = np.exp(logits - np.max(logits))\n",
    "        p = exp_logits / exp_logits.sum()\n",
    "\n",
    "        # draw a sample\n",
    "        last_char_id = rng.choice(np.arange(K), p=p)\n",
    "        generated.append(index_to_char[last_char_id])\n",
    "\n",
    "    return start_string + ''.join(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b66e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 53247/53248 [54:07<00:00, 17.89it/s]  2025-05-10 19:07:14.804118: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "100%|██████████| 53248/53248 [54:07<00:00, 16.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 — avg loss: 1.8653\n",
      "ROMEO. a sweet to these to prose and enther?\n",
      "That I’ll I to said did soft in dispies to gentlems\n",
      "un lutt me to cell no my since here as imperes rease,\n",
      "Since my may, what and mell beared come;\n",
      "And all it not th’ angar’d so so powar.\n",
      "\n",
      " ANEONIO.\n",
      "I smeange. Well cloudly but with but of recoy,\n",
      "“Aways flowen I \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "init_lstm1 = initialize_lstm(1, 100)\n",
    "\n",
    "print('Generated text pre-training:')\n",
    "print(sample_text_lstm(init_lstm1, 'ROMEO.', 300))\n",
    "print()\n",
    "\n",
    "params = {\n",
    "    'eta': 0.001,\n",
    "    'num_epochs': 1,\n",
    "    'n_batch': 100,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.999,\n",
    "    'eps': 1e-8\n",
    "}\n",
    "\n",
    "lstm1 = train_lstm_adam(dataset, init_lstm1, params)\n",
    "\n",
    "print('Generated text post-training:')\n",
    "print(sample_text_lstm(lstm1, 'ROMEO.', 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ec5fcd",
   "metadata": {},
   "source": [
    "## Quantitative and Qualitative comparison between LSTM and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d178aee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "407fce9f",
   "metadata": {},
   "source": [
    "## Optimizing performance of LSTM\n",
    "* Hyperparameter tuning, different ways of regularization\n",
    "* Temperature and Nucleus sampling\n",
    "* Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1359f857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff8e6004",
   "metadata": {},
   "source": [
    "## Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e864e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acf18c1b",
   "metadata": {},
   "source": [
    "## BPE Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc3781a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5b93b35",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8237ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
