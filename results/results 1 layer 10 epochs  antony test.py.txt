antonyzhang@MacBook-Pro test1 % conda activate DD2424
(DD2424) antonyzhang@MacBook-Pro test1 % python test.py        
2025-05-17 17:02:37.225954: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max
2025-05-17 17:02:37.225994: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB
2025-05-17 17:02:37.225999: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1747494157.226013 4659102 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
I0000 00:00:1747494157.226038 4659102 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
Input batch shape: (64, 100)
Target batch shape: (64, 100)
Decoded input: e.
My fleet hath yielded to the foe, and yonder
They cast their caps up and carouse together
Like fr
Decoded target: .
My fleet hath yielded to the foe, and yonder
They cast their caps up and carouse together
Like fri
Model: "transformer_model"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ token_and_position_embedding         │ ?                           │          13,568 │
│ (TokenAndPositionEmbedding)          │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ transformer_block (TransformerBlock) │ ?                           │         330,240 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ transformer_block_1                  │ ?                           │         330,240 │
│ (TransformerBlock)                   │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ transformer_block_2                  │ ?                           │         330,240 │
│ (TransformerBlock)                   │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ transformer_block_3                  │ ?                           │         330,240 │
│ (TransformerBlock)                   │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ transformer_block_4                  │ ?                           │         330,240 │
│ (TransformerBlock)                   │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ transformer_block_5                  │ ?                           │         330,240 │
│ (TransformerBlock)                   │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dropout_12 (Dropout)                 │ ?                           │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_13 (Dense)                     │ (64, 100, 106)              │          13,674 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 2,008,682 (7.66 MB)
 Trainable params: 2,008,682 (7.66 MB)
 Non-trainable params: 0 (0.00 B)

Training Transformer model...
Epoch 1/10
2025-05-17 17:02:45.553179: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.
582/582 ━━━━━━━━━━━━━━━━━━━━ 410s 678ms/step - accuracy: 0.3025 - loss: 2.9923 - val_accuracy: 0.4793 - val_loss: 1.9988
Epoch 2/10
582/582 ━━━━━━━━━━━━━━━━━━━━ 394s 676ms/step - accuracy: 0.7694 - loss: 0.8462 - val_accuracy: 0.9901 - val_loss: 0.0424
Epoch 3/10
582/582 ━━━━━━━━━━━━━━━━━━━━ 397s 680ms/step - accuracy: 0.9523 - loss: 0.1796 - val_accuracy: 0.9908 - val_loss: 0.0364
Epoch 4/10
582/582 ━━━━━━━━━━━━━━━━━━━━ 401s 688ms/step - accuracy: 0.9877 - loss: 0.0493 - val_accuracy: 0.9914 - val_loss: 0.0355
Epoch 5/10
582/582 ━━━━━━━━━━━━━━━━━━━━ 408s 700ms/step - accuracy: 0.9887 - loss: 0.0445 - val_accuracy: 0.9913 - val_loss: 0.0355
Epoch 6/10
582/582 ━━━━━━━━━━━━━━━━━━━━ 414s 710ms/step - accuracy: 0.9892 - loss: 0.0424 - val_accuracy: 0.9913 - val_loss: 0.0351
Epoch 7/10
582/582 ━━━━━━━━━━━━━━━━━━━━ 436s 748ms/step - accuracy: 0.9895 - loss: 0.0411 - val_accuracy: 0.9911 - val_loss: 0.0363
Epoch 8/10
582/582 ━━━━━━━━━━━━━━━━━━━━ 444s 761ms/step - accuracy: 0.9897 - loss: 0.0407 - val_accuracy: 0.9915 - val_loss: 0.0341
Epoch 9/10
582/582 ━━━━━━━━━━━━━━━━━━━━ 460s 789ms/step - accuracy: 0.9899 - loss: 0.0399 - val_accuracy: 0.9915 - val_loss: 0.0350
Epoch 10/10
582/582 ━━━━━━━━━━━━━━━━━━━━ 469s 803ms/step - accuracy: 0.9899 - loss: 0.0394 - val_accuracy: 0.9915 - val_loss: 0.0348
Generated text from Transformer (pre-training, if epochs=0):
The Pra i rantteoHd s? hvayosoopoogur h h y
ekkdstos nLyetmgaelc efc.bpaepgi ,oatt
  a mwLso,gh essca  g,fdhde.ho iy
  ed uh- hoCedlohose et nSeLkyToR oitconsoeoindte s ooE so,hsma  
syl  t
aatreayb.ehha,i
to oSoao gC tntrdt c ssoekanAld totI ola , emocntbL f
 t

    ath Ill mc e ;srpr mnweaeiscatoRm hoba

Transformer model weights saved.
2025-05-17 18:13:57.349 python[29648:4659102] +[IMKClient subclass]: chose IMKClient_Modern
2025-05-17 18:13:57.349 python[29648:4659102] +[IMKInputSession subclass]: chose IMKInputSession_Modern
2025-05-17 18:19:21.881 python[29648:4659102] The class 'NSSavePanel' overrides the method identifier.  This method is implemented by class 'NSWindow'

Evaluating Transformer model on test set...
124/124 ━━━━━━━━━━━━━━━━━━━━ 34s 271ms/step - accuracy: 0.9915 - loss: 0.0351
Transformer Test Loss: 0.0378
Transformer Test Accuracy: 0.9909

Generating text with trained Transformer for qualitative evaluation (standard sampling):
ROMEO:tlnasfhhiadapaiAstieeaohgaesserhiyaeacshaaaiasaaihlehvaa lasasisoauiaaiitsmsaheaidslaalrcalalwiaidniiwabac lsssciadmlysseawsaiamilegananstsynagsssUalcaDssaiasmnoaaaehyrhsa aftsunl ssa
 aacte taaasbshaeasaislasmananmainmsmallssaeyrantlrisymstlesaahsledcwialsmm hlrhveiaaeainasisetrOlyaa mehe lnasOhnlh

Generating text with trained Transformer (Nucleus Sampling):
ROMEO:eoaeyOacaheoolyayessshylaatsayaaaaeaassssaieaaaysh saasssselhloealcaaanmemsaadaetseashes aelassashayaiaastadaaetyayisayydoaassaaahrssaesnssdlc mnaiyoiyaeeliaimsmadssnwasaialadacs esawwascocoyeelaadiaeiymi cretdlee aaahasnaheiar esdaeoiosestaeaaaeseeaemsasieaesaaatseeysayllaadcceeaahlstaslaaelstalssn

Quantitative Evaluation for Transformer (Nucleus Sampled Text):
Transformer Model (Nucleus) -> Spelling Accuracy: 0.11, Bigram Overlap: 0.96, Trigram Diversity: 0.76
(DD2424) antonyzhang@MacBook-Pro test1 % 